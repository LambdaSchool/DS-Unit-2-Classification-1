{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading Datasets using Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv train_features.csv\r\n",
      "test_features.csv     train_labels.csv\r\n"
     ]
    }
   ],
   "source": [
    "#!kaggle competitions download -c ds4-predictive-modeling-challenge\n",
    "#!unzip train_f*.zip\n",
    "#!unzip test*.zip\n",
    "#!unzip train_l*.zip\n",
    "#!rm *.zip\n",
    "#!chmod +r test_f*.csv\n",
    "#!chmod +r train*.csv\n",
    "#!chmod +r train_l*.csv\n",
    "!ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading CSVs into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DF we will work with\n",
    "\n",
    "We will ignore the test_df since it is not part of the process in creating a working model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "features = pd.read_csv('train_features.csv')\n",
    "\n",
    "label = 'status_group'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation and holdout sets\n",
    "\n",
    "The question we have is whether we should grab our validation and holdout set randomly or selectively?\n",
    "https://www.fast.ai/2017/11/13/validation-sets/\n",
    "https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/8\n",
    "\n",
    "For this problem, we will do with randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = tts(features,\n",
    "                                     train_labels[label],\n",
    "                                     train_size=0.60,\n",
    "                                     test_size=0.40,\n",
    "                                     stratify=labels,\n",
    "                                     random_state=42\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_hold, y_val, y_hold = tts(x_val,\n",
    "                                   y_val, \n",
    "                                   train_size=0.60, \n",
    "                                   test_size = 0.40,\n",
    "                                   stratify=y_val,\n",
    "                                   random_state=42\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our 3 Datasets we will be working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[0], x_val.shape[0], x_hold.shape[0]\n",
    "dfs = [x_train, x_val, x_hold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Special case of near 0 value instead of 0\n",
    "    df['latitude'] = df['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # 0s in these columns are meant as NaN.\n",
    "    cols = ['construction_year', 'longitude', 'latitude']\n",
    "    for col in cols:\n",
    "        df[col] = df[col].replace(0, np.nan)\n",
    "#!!!!!!!!!!!!!!!!! EXPERIMENT WITH MEDIAN INSTEAD !!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "#!!!!!!!!!!!!!!!!! EXPERIMENT WITH MEDIAN INSTEAD !!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    # Convert date_recorded to datetime\n",
    "    df['date_recorded'] = pd.to_datetime(df['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract year from date_recorded\n",
    "    df['year_recorded'] = df['date_recorded'].dt.year\n",
    "    \n",
    "    # quantity & quantity_group are duplicates, so drop one\n",
    "    df = df.drop(columns='quantity_group')\n",
    "    \n",
    "    # for categoricals with missing values, fill with the category 'MISSING'\n",
    "    categoricals = df.select_dtypes(exclude='number').columns\n",
    "    for col in categoricals:\n",
    "        df[col] = df[col].fillna('MISSING')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1, training df\n",
    "\n",
    "#### Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled=[]\n",
    "stage = 1\n",
    "for i in range(stage):\n",
    "    wrangled.append(wrangle(dfs[i]))\n",
    "    \n",
    "x_train = wrangled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting categorical and continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe with all train columns except the target & id\n",
    "x_train_c = x_train.drop(columns=['id'])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = x_train_c.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = x_train_c.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features\n",
    "\n",
    "# Training df ready for pre-processing\n",
    "x_train = x_train[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catergorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encode categoricals of train\n",
    "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "x_train_e = encoder.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling and Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler: fit_transform on train, transform on val & test\n",
    "scaler = RobustScaler()\n",
    "x_train_s = scaler.fit_transform(x_train_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='auto', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "model.fit(x_train_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The quick win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    df = wrangle(df)\n",
    "    df = df.drop(columns=['id'])\n",
    "    numeric_features = df.select_dtypes(include='number').columns.tolist()\n",
    "    cardinality = df.select_dtypes(exclude='number').nunique()\n",
    "    categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
    "    features=numeric_features + categorical_features\n",
    "    df = df[features]\n",
    "    encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "    scaler = RobustScaler()\n",
    "    df = scaler.fit_transform(encoder.fit_transform(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(pre_processing(test_features))\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('submission-02.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

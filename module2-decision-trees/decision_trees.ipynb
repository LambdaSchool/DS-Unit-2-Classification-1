{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_trees.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX0jgn6FTo2I",
        "colab_type": "text"
      },
      "source": [
        "_Lambda School Data Science, Classification 1_\n",
        "\n",
        "This sprint, your project is about water pumps in Tanzania. Can you predict which water pumps are faulty?\n",
        "\n",
        "# Decision Trees, Data Cleaning\n",
        "\n",
        "#### Objectives\n",
        "- clean data with outliers\n",
        "- impute missing values\n",
        "- use scikit-learn for decision trees\n",
        "- understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions\n",
        "- get and interpret feature importances of a tree-based model\n",
        "\n",
        "#### Links\n",
        "\n",
        "- A Visual Introduction to Machine Learning\n",
        "  - [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "  - [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
        "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) — _Don’t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
        "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MqSTNFg8ev4",
        "colab_type": "text"
      },
      "source": [
        "### OPTIONAL SETUP\n",
        "\n",
        "#### 1. Downgrade pandas to fix pivot table bug\n",
        "\n",
        "For this lesson, I'll downgrade pandas from 0.24 to 0.23.4, because of a known issue: https://github.com/pandas-dev/pandas/issues/25087\n",
        "\n",
        "I'm making a pivot table just for demonstration during this lesson, but it's not required for your assignment. So, you don't need to downgrade pandas if you don't want to.\n",
        "\n",
        "#### 2. Install graphviz to visualize trees\n",
        "\n",
        "This is also not required for your assignment.\n",
        "\n",
        "Anaconda:  \n",
        "```\n",
        "conda install python-graphviz\n",
        "```\n",
        "\n",
        "Google Colab:  \n",
        "```\n",
        "!pip install graphviz\n",
        "!apt-get install graphviz\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxCNx-MTySTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pandas==0.23.4\n",
        "# !pip install graphviz\n",
        "# !apt-get install graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFGmW4ijn4YN",
        "colab_type": "text"
      },
      "source": [
        "## Clean data with outliers, impute missing values (example solutions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt_SrDYHT4jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg8hVHaJTldG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv('https://drive.google.com/uc?export=download&id=14ULvX0uOgftTB2s97uS8lIx1nHGQIB0P'), \n",
        "                 pd.read_csv('https://drive.google.com/uc?export=download&id=1r441wLr7gKGHGLyPpKauvCuUOU556S2f'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv('https://drive.google.com/uc?export=download&id=1wvsYl9hbRbZuIuoaLWCsW_kbcxCdocHz')\n",
        "sample_submission = pd.read_csv('https://drive.google.com/uc?export=download&id=1kfJewnmhowpUo381oSn3XqsQ6Eto23XV')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "# Print dataframe shapes\n",
        "print('train', train.shape)\n",
        "print('val', val.shape)\n",
        "print('test', test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeIQ1lDmqAHY",
        "colab_type": "text"
      },
      "source": [
        "Some of the locations are at [\"Null Island\"](https://en.wikipedia.org/wiki/Null_Island) instead of Tanzania."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVltyo5bpac7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(x='longitude', y='latitude', data=train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-OOtTMzqkhM",
        "colab_type": "text"
      },
      "source": [
        "#### Define a function to wrangle train, validate, and test sets in the same way.\n",
        "\n",
        "Fix the location, and do more data cleaning and feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m81UBUsiaIzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace them with the column mean.\n",
        "    cols_with_zeros = ['construction_year', 'longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col] = X[col].fillna(X[col].mean())\n",
        "        \n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract year from date_recorded\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # for categoricals with missing values, fill with the category 'MISSING'\n",
        "    categoricals = X.select_dtypes(exclude='number').columns\n",
        "    for col in categoricals:\n",
        "        X[col] = X[col].fillna('MISSING')\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMqlStuWqwde",
        "colab_type": "text"
      },
      "source": [
        "Now the locations look better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoIkLqHLpq3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.relplot(x='longitude', y='latitude', hue='status_group', \n",
        "            data=train, alpha=0.1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypjt052Lrmgn",
        "colab_type": "text"
      },
      "source": [
        "#### Select features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "666FUbXOXgL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target & id\n",
        "train_features = train.drop(columns=[target, 'id'])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moC9WikFrqJV",
        "colab_type": "text"
      },
      "source": [
        "#### Encode categoricals, scale features, fit and score Logistic Regression model, make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUUq8Zt5X1bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector \n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]\n",
        "\n",
        "# Encoder: fit_transform on train, transform on val & test\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "# Scaler: fit_transform on train, transform on val & test\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "X_val_scaled = scaler.transform(X_val_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_encoded)\n",
        "\n",
        "# Model: Fit on train, score on val, predict on test\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print('Validation Accuracy', model.score(X_val_scaled, y_val))\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Write submission csv file\n",
        "submission = sample_submission.copy()\n",
        "submission['status_group'] = y_pred\n",
        "submission.to_csv('submission-02.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EQOYZtqr00F",
        "colab_type": "text"
      },
      "source": [
        "#### Get and plot coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFlfjFN1gyjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coefficients = pd.Series(model.coef_[0], X_train_encoded.columns)\n",
        "plt.figure(figsize=(10,30))\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMW9JbD6wZ28",
        "colab_type": "text"
      },
      "source": [
        "## Use scikit-learn for decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s7t37euwd41",
        "colab_type": "text"
      },
      "source": [
        "### Compare a Logistic Regression with 2 features, longitude & latitude ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44TUfQpeo64d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD31msBkwlVQ",
        "colab_type": "text"
      },
      "source": [
        "### ... versus a Decision Tree Classifier with 2 features, longitude & latitude\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xIa_1Kozfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkV4TjcNAPzp",
        "colab_type": "text"
      },
      "source": [
        "## Understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxBwjWHq_0xK",
        "colab_type": "text"
      },
      "source": [
        "### Visualize decision tree\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXF2Jkl4_3Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5GYg5Ze7de3",
        "colab_type": "text"
      },
      "source": [
        "### Make 3 heatmaps, with longitude & latitude\n",
        "- Actual % of functional waterpumps\n",
        "- Decision Tree predicted probability of functional waterpumps\n",
        "- Logistic Regression predicted probability of functional waterpumps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6uVMTX3u7fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zlKegIZBbGr",
        "colab_type": "text"
      },
      "source": [
        "### Interlude #1: predicting golf putts\n",
        "(1 feature, non-linear, regression)\n",
        "\n",
        "https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjvPHk-FBayo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['distance', 'tries', 'successes']\n",
        "data = [[2, 1443, 1346],\n",
        "        [3, 694, 577],\n",
        "        [4, 455, 337],\n",
        "        [5, 353, 208],\n",
        "        [6, 272, 149],\n",
        "        [7, 256, 136],\n",
        "        [8, 240, 111],\n",
        "        [9, 217, 69],\n",
        "        [10, 200, 67],\n",
        "        [11, 237, 75],\n",
        "        [12, 202, 52],\n",
        "        [13, 192, 46],\n",
        "        [14, 174, 54],\n",
        "        [15, 167, 28],\n",
        "        [16, 201, 27],\n",
        "        [17, 195, 31],\n",
        "        [18, 191, 33],\n",
        "        [19, 147, 20],\n",
        "        [20, 152, 24]]\n",
        "\n",
        "putts = pd.DataFrame(columns=columns, data=data)\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr-83jOEBsxK",
        "colab_type": "text"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x988kCamBpbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']\n",
        "lr = LinearRegression()\n",
        "lr.fit(putts_X, putts_y)\n",
        "print('R^2 Score', lr.score(putts_X, putts_y))\n",
        "ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "ax.plot(putts_X, lr.predict(putts_X));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLGrZjfFGD-",
        "colab_type": "text"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpFIetrsB2yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import graphviz\n",
        "from ipywidgets import interact\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "\n",
        "def viztree(decision_tree, feature_names):\n",
        "    dot_data = export_graphviz(decision_tree, out_file=None, feature_names=feature_names, \n",
        "                               filled=True, rounded=True)   \n",
        "    return graphviz.Source(dot_data)\n",
        "\n",
        "def putts_tree(max_depth=1):\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "    tree.fit(putts_X, putts_y)\n",
        "    print('R^2 Score', tree.score(putts_X, putts_y))\n",
        "    ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "    ax.step(putts_X, tree.predict(putts_X), where='mid')\n",
        "    plt.show()\n",
        "    display(viztree(tree, feature_names=['distance']))\n",
        "\n",
        "interact(putts_tree, max_depth=(1,6,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBQJB0PVFtIE",
        "colab_type": "text"
      },
      "source": [
        "### Interlude #2: Simple housing \n",
        "(2 features, regression)\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHQ6oh1uFzHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['Price', 'Good Location', 'Big Size']\n",
        "\n",
        "data = [[300000, 1, 1], \n",
        "        [200000, 1, 0], \n",
        "        [250000, 0, 1], \n",
        "        [150000, 0, 0]]\n",
        "\n",
        "house = pd.DataFrame(columns=columns, data=data)\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH8IVe6GGAsm",
        "colab_type": "text"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9uMiKdzF6Ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7AASlWVGMD4",
        "colab_type": "text"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWlqO_usGKS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZkaOREUGzYp",
        "colab_type": "text"
      },
      "source": [
        "### Simple housing, with a twist: _Feature Interaction_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HE0_pibGzJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "house.loc[0, 'Price'] = 400000\n",
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh7ZJe7GHCSp",
        "colab_type": "text"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYINxJkdG_Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KIVsFO_HSmS",
        "colab_type": "text"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6JYZBZBHIX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUW3dabWAWPk",
        "colab_type": "text"
      },
      "source": [
        "## Get and interpret feature importances of a tree-based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1_grdR-AVyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOgglh_kCt_c",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "- Start a clean notebook, or continue with yesterday's assignment notebook.\n",
        "- Continue to participate in our Kaggle competition with the Tanzania Waterpumps data. \n",
        "- Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n",
        "- Try a Decision Tree Classifier. \n",
        "- Submit new predictions.\n",
        "- Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "## Stretch Goals\n",
        "- Create visualizations and share on Slack.\n",
        "- Read more about decision trees and tree ensembles. You can start with the links at the top of this notebook.\n",
        "- Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html):\n",
        "\n",
        "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "\n",
        "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
        "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n"
      ]
    }
  ]
}
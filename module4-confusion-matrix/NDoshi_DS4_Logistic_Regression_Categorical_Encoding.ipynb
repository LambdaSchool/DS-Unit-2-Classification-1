{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "%matplotlib inline\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 41), (14358, 40), (14358, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data from sources\n",
    "train = pd.merge(pd.read_csv('https://drive.google.com/uc?export=download&id=14ULvX0uOgftTB2s97uS8lIx1nHGQIB0P'),\n",
    "                 pd.read_csv('https://drive.google.com/uc?export=download&id=1r441wLr7gKGHGLyPpKauvCuUOU556S2f'))\n",
    "test = pd.read_csv('https://drive.google.com/uc?export=download&id=1wvsYl9hbRbZuIuoaLWCsW_kbcxCdocHz')\n",
    "submission = pd.read_csv('https://drive.google.com/uc?export=download&id=1kfJewnmhowpUo381oSn3XqsQ6Eto23XV')\n",
    "train.shape, test.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data in train and validate\n",
    "train, val = train_test_split(train, train_size=0.85, test_size=0.15, \n",
    "                              stratify=train['status_group'], random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to wrangle data\n",
    "def wrangle(X):\n",
    "    X_Clean = X.copy() # Create copy to not pass changes to main df\n",
    "    \n",
    "    # Convert region_code and district_code variables to str\n",
    "    X_Clean['region_code'] = X_Clean['region_code'].astype('str')\n",
    "    X_Clean['district_code'] = X_Clean['district_code'].astype('str')\n",
    "    \n",
    "    # Convert all NaN values to unknown as they are all categorical variables\n",
    "    X_Clean = X_Clean.fillna('unknown')\n",
    "    \n",
    "    # Drop columns not needed and are duplicates\n",
    "    X_Clean = X_Clean.drop(columns=['id','recorded_by','quantity_group','payment_type'])\n",
    "    \n",
    "    # Convert date_recorded column to show year\n",
    "    dates = pd.to_datetime(X_Clean['date_recorded'])\n",
    "    X_Clean['date_recorded'] = dates.dt.year\n",
    "    \n",
    "    # Convert 0's to NaN in construction year, gps height, logitude and latitude\n",
    "    numericals = ['gps_height','longitude','construction_year', 'latitude', 'population']\n",
    "    for col in numericals:\n",
    "        X_Clean[col] = X_Clean[col].replace(0, np.nan)\n",
    "        \n",
    "    # Convert latitude almost 0 to nan\n",
    "    X_Clean['latitude'] = X_Clean['latitude'].replace(X_Clean['latitude'].max(), np.nan)\n",
    "    \n",
    "    # Update missing numerical values using ward as basis\n",
    "    for col in numericals:\n",
    "        replacements = X_Clean.groupby('ward')[col].transform('mean')\n",
    "        X_Clean[col] = X_Clean[col].fillna(replacements)\n",
    "    \n",
    "    # Now the numericals have NaN values, I will replace with means associated with region\n",
    "    for col in numericals:\n",
    "        replacements = X_Clean.groupby('region')[col].transform('mean')\n",
    "        X_Clean[col] = X_Clean[col].fillna(replacements)\n",
    "        \n",
    "    # Any leftover numerical features with NaN will be updated with mean\n",
    "    #for col in numericals:\n",
    "      #   replacements = X_Clean[col].mean()\n",
    "      #  X_Clean[col] = X_Clean[col].fillna(replacements)\n",
    "    \n",
    "    for col in numericals:\n",
    "        dist = X_Clean[col].value_counts(normalize=True)\n",
    "        X_Clean.loc[X_Clean[col].isna(), col] = np.random.choice(dist.index,\n",
    "                                                                 size=X_Clean[col].isna().sum(),\n",
    "                                                                 p=dist.values)\n",
    "        \n",
    "    # Create new feature age, based on date recorded minus construction date\n",
    "    X_Clean['age'] = X_Clean['date_recorded'] - X_Clean['construction_year']\n",
    "        \n",
    "    # Return cleaned df\n",
    "    return X_Clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run wrangle function on train, val, and test sets\n",
    "train_c = wrangle(train)\n",
    "val_c = wrangle(val)\n",
    "test_c = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Get a dataframe with all train columns except the target\n",
    "train_features = train_c.drop(columns=[target])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 250].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X matrices and Y target vectors\n",
    "X_train = train_c[features]\n",
    "Y_train = train_c[target]\n",
    "X_val = val_c[features]\n",
    "Y_val = val_c[target]\n",
    "X_test = test_c[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pipeline method to train decision tree classifier model\n",
    "pipeline1 = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    SimpleImputer(strategy='mean'), \n",
    "    DecisionTreeClassifier(max_depth=20, random_state=82)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val, predict on test\n",
    "pipeline1.fit(X_train, Y_train)\n",
    "print('Validation Accuracy', pipeline1.score(X_val, Y_val))\n",
    "y_pred = pipeline1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.8138047138047138\n"
     ]
    }
   ],
   "source": [
    "# Use pipeline method to train random forest classifier model\n",
    "pipeline2 = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=23, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val\n",
    "pipeline2.fit(X_train, Y_train)\n",
    "print('Validation Accuracy', pipeline2.score(X_val, Y_val))\n",
    "y_pred2 = pipeline2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using Random Search CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random search to baseline model\n",
    "rf = RandomForestClassifier()\n",
    "random_p2 = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encoding and scaling to train and validation data to train data set to run through model training\n",
    "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.transform(X_val)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_val_scaled = scaler.transform(X_val_encoded)\n",
    "\n",
    "# Fit random search model\n",
    "random_p2.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Score model against validation data set\n",
    "print('Validation Accuracy', random_p2.score(X_val_scaled, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "sub = submission.copy()\n",
    "sub['status_group'] = y_pred2\n",
    "sub.to_csv('NDoshi_DS4_Sub3.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
